## Citation impact

Citation impact is a measure of how many times an academic journal article or book or author is cited by other articles,
books or authors. Citation counts are interpreted as measures of the impact or influence of academic work and have given rise to the field
of bibliometrics or scientometrics, specializing in the study of patterns of academic impact through citation analysis. The journal impact
factor, the two-year average ratio of citations to articles published, is a measure of the importance of journals. It is used by academic
institutions in decisions about academic tenure, promotion and hiring, and hence also used by authors in deciding which journal to publish in. Citation-like measures are also used in other fields that do ranking, such as Google's PageRank algorithm, software metrics, college and university rankings, and business performance indicators.


## Article-level

One of the most basic citation metrics is how often an article was cited in other articles, books, or other sources (such as theses). 
Citation rates are heavily dependent on the discipline and the number of people working in that area. For instance, many more scientists work in neuroscience than in mathematics, and neuroscientists publish more papers than mathematicians, hence neuroscience papers are much more often cited than papers in mathematics. Similarly, review papers are more often cited than regular research papers because they summarize results from many papers. This may also be the reason why papers with shorter titles get more citations, given that they are usually covering a broader area.

## Most-cited papers

The most-cited paper in history is a paper by Oliver Lowry describing an assay to measure the concentration of proteins. By 2014 it had accumulated more than 305,000 citations. The 10 most cited papers all had more than 40,000 citations. To reach the top-100 papers required 12,119 citations by 2014. Of Thomson Reuter's Web of Science database with more than 58 million items only 14,499 papers (~0.026%) had more than 1,000 citations in 2014.

## Journal-level

The simplest journal-level metric is the journal impact factor (JIF), the average number of citations that articles published by a journal in the previous two years have received in the current year, as calculated by Clarivate. Other companies report similar metrics, such as the CiteScore (CS), based on Scopus.

However, very high JIF or CS are often based on a small number of very highly cited papers. For instance, most papers in Nature (impact factor 38.1, 2016) were only cited 10 or 20 times during the reference year (see figure). Journals with a lower impact (e.g. PLOS ONE, impact factor 3.1) publish many papers that are cited 0 to 5 times but few highly cited articles.

Journal-level metrics are often misinterpreted as a measure for journal quality or article quality. They are not an article-level metric, hence its use to determine the impact of a single article is statistically invalid. Citation distribution is skewed for journals because a very small number of articles is driving the vast majority of citations; therefore, some journals have stopped publicizing their impact factor, e.g. the journals of the American Society for Microbiology.

More elaborate journal-level metrics include the Eigenfactor, and the SCImago Journal Rank.

## Author-level

Total citations, or average citation count per article, can be reported for an individual author or researcher. Many other measures have been proposed, beyond simple citation counts, to better quantify an individual scholar's citation impact. The best-known measures include the h-index and the g-index. Each measure has advantages and disadvantages, spanning from bias to discipline-dependence and limitations of the citation data source. Counting the number of citations per paper is also employed to identify the authors of citation classics.

Citations are distributed highly unequally among researchers. In a study based on the Web of Science database across 118 scientific disciplines, the top 1% most-cited authors accounted for 21% of all citations. Between 2000 and 2015, the proportion of citations that went to this elite group grew from 14% to 21%. The highest concentrations of ‘citation elite’ researchers were in the Netherlands, the United Kingdom, Switzerland and Belgium. Note that 70% of the authors in the Web of Science database have fewer than 5 publications, so that the most-cited authors among the 4 million included in this study constitute a tiny fraction.

## Alternatives

An alternative approach to measure a scholar's impact relies on usage data, such as number of downloads from publishers and analyzing citation performance, often at article level.

As early as 2004, the BMJ published the number of views for its articles, which was found to be somewhat correlated to citations. In 2008 the Journal of Medical Internet Research began publishing views and Tweets. These "tweetations" proved to be a good indicator of highly cited articles, leading the author to propose a "Twimpact factor", which is the number of Tweets it receives in the first seven days of publication, as well as a Twindex, which is the rank percentile of an article's Twimpact factor.

In response to growing concerns over the inappropriate use of journal impact factors in evaluating scientific outputs and scientists themselves, Université de Montréal, Imperial College London, PLOS, eLife, EMBO Journal, The Royal Society, Nature and Science proposed citation distributions metrics as alternative to impact factors.

## Open Access publications

Open access (OA) publications are accessible without cost to readers, hence they would be expected to be cited more frequently. Some experimental and observational studies have found that articles published in OA journals do not receive more citations, on average, than those published in subscription journals; other studies have found that they do.

The evidence that author-self-archived ("green") OA articles are cited more than non-OA articles is somewhat stronger than the evidence that ("gold") OA journals are cited more than non-OA journals. Two reasons for this are that many of the top-cited journals today are still only hybrid OA (author has the option to pay for gold) and many pure author-pays OA journals today are either of low quality or downright fraudulent "predatory journals," preying on authors' eagerness to publish-or-perish, thereby lowering the average citation counts of OA journals.

## Recent developments

An important recent development in research on citation impact is the discovery of universality, or citation impact patterns that hold across different disciplines in the sciences, social sciences, and humanities. For example, it has been shown that the number of citations received by a publication, once properly rescaled by its average across articles published in the same discipline and in the same year, follows a universal log-normal distribution that is the same in every discipline. This finding has suggested a universal citation impact measure that extends the h-index by properly rescaling citation counts and resorting publications, however the computation of such a universal measure requires the collection of extensive citation data and statistics for every discipline and year. Social crowdsourcing tools such as Scholarometer have been proposed to address this need. Kaur et al. proposed a statistical method to evaluate the universality of citation impact metrics, i.e., their capability to compare impact fairly across fields. Their analysis identifies universal impact metrics, such as the field-normalized h-index.

Research suggests the impact of an article can be, partly, explained by superficial factors and not only by the scientific merits of an article. Field-dependent factors are usually listed as an issue to be tackled not only when comparison across disciplines are made, but also when different fields of research of one discipline are being compared. For instance in Medicine among other factors the number of authors, the number of references, the article length, and the presence of a colon in the title influence the impact. Whilst in Sociology the number of references, the article length, and title length are among the factors. Also it is found that scholars engage in ethically questionable behavior in order to inflate the number of citations articles receive.

Automated citation indexing has changed the nature of citation analysis research, allowing millions of citations to be analyzed for large scale patterns and knowledge discovery. The first example of automated citation indexing was CiteSeer, later to be followed by Google Scholar. More recently, advanced models for a dynamic analysis of citation aging have been proposed. The latter model is even used as a predictive tool for determining the citations that might be obtained at any time of the lifetime of a corpus of publications.

Some researchers also propose that the journal citation rate on Wikipedia, next to the traditional citation index, "may be a good indicator of the work’s impact in the field of psychology."

According to Mario Biagioli: "All metrics of scientific evaluation are bound to be abused. Goodhart's law states that when a feature of the economy is picked as an indicator of the economy, then it inexorably ceases to function as that indicator because people start to game it."


#### francais :

## Incidence du nombre de citations

L’impact des citations est une mesure du nombre de fois qu’un article de revue académique ou un livre ou un auteur est cité par d’autres articles,
livres ou auteurs. Les citations sont interprétées comme des mesures de l’impact ou de l’influence du travail universitaire et ont donné naissance au domaine
de bibliométrie ou de scientométrie, spécialisée dans l’étude des modèles d’impact académique par l’analyse des citations.
Le rapport moyen de deux ans entre les citations et les articles publiés est une mesure de l’importance des
institutions dans les décisions concernant la permanence académique, la promotion et l’embauche, et donc également utilisé par les auteurs pour décider dans quelle revue publier. Des mesures semblables à des citations sont également utilisées dans d’autres domaines qui font le classement, tels que l’algorithme PageRank de Google, les mesures logicielles, les classements collégiaux et universitaires, et les indicateurs de performance des entreprises.

## Niveau article

L’une des mesures de citation les plus élémentaires est la fréquence à laquelle un article a été cité dans d’autres articles, livres ou autres sources (comme des thèses). 
Le taux de citation dépend beaucoup de la discipline et du nombre de personnes qui travaillent dans ce domaine. Par exemple, beaucoup plus de scientifiques travaillent en neurosciences qu’en mathématiques, et les neuroscientifiques publient plus d’articles que les mathématiciens, d’où les articles sur les neurosciences sont beaucoup plus souvent cités que les articles en mathématiques. De même, les articles de synthèse sont plus souvent cités que les articles de recherche réguliers parce qu’ils résument les résultats de nombreux articles. Cela peut également être la raison pour laquelle les articles avec des titres plus courts obtiennent plus de citations, étant donné qu’ils couvrent généralement un domaine plus large.



## Articles les plus cités

Le papier le plus cité dans l’histoire est un papier par Oliver Lowry décrivant un essai pour mesurer la concentration des protéines. En 2014, il avait accumulé plus de 305 000 citations. Les 10 articles les plus cités comptaient tous plus de 40 000 citations. Pour atteindre le top-100 des articles requis 12,119 citations d’ici 2014. Sur la base de données Web of Science de Thomson Reuter avec plus de 58 millions d’articles, seuls 14499 articles (~0,026 %) ont reçu plus de 1000 citations en 2014.

## Niveau journal

La mesure la plus simple au niveau des revues est le facteur d’impact des revues (JIF), le nombre moyen de citations que les articles publiés par une revue au cours des deux années précédentes ont reçues au cours de l’année en cours, calculé par Clarivate. D’autres entreprises font état de mesures similaires, comme le CiteScore (CS), basé sur Scopus.

Cependant, les JIF ou CS très élevés sont souvent basés sur un petit nombre d’articles très cités. Par exemple, la plupart des articles de Nature (facteur d’impact 38.1, 2016) n’ont été cités que 10 ou 20 fois au cours de l’année de référence (voir la figure). Les revues ayant un impact moindre (p. ex., PLOS ONE, facteur d’impact 3.1) publient de nombreux articles qui sont cités 0 à 5 fois, mais peu d’articles fortement cités.

Les mesures au niveau des revues sont souvent mal interprétées comme une mesure de la qualité des revues ou des articles. Ils ne sont pas une mesure au niveau de l’article, d’où leur utilisation pour déterminer l’impact d’un seul article est statistiquement invalide. La distribution des citations est biaisée pour les revues parce qu’un très petit nombre d’articles alimente la grande majorité des citations; par conséquent, certaines revues ont cessé de faire connaître leur facteur d’impact, p. ex., les revues de l’American Society for Microbiology.

Les mesures plus élaborées au niveau des revues comprennent l’Eigenfactor et le SCImago Journal Rank.

## Niveau auteur

Le nombre total de citations, ou le nombre moyen de citations par article, peut être déclaré pour un auteur ou un chercheur. De nombreuses autres mesures ont été proposées, au-delà du simple dénombrement des citations, pour mieux quantifier l’impact des citations individuelles. Les mesures les plus connues sont l’indice h et l’indice g. Chaque mesure présente des avantages et des inconvénients, allant du biais à la discipline-dépendance et aux limites de la source de données de citation. Le nombre de citations par article est également utilisé pour identifier les auteurs de citations classiques.

Les citations sont distribuées de façon très inégale entre les chercheurs. Dans une étude fondée sur la base de données du Web of Science de 118 disciplines scientifiques, les auteurs les plus cités représentaient 21 % de toutes les citations. Entre 2000 et 2015, la proportion de citations adressées à ce groupe d’élite est passée de 14 % à 21 %. Les concentrations les plus élevées de chercheurs « d’élite des citations » se trouvaient aux Pays-Bas, au Royaume-Uni, en Suisse et en Belgique. Il est à noter que 70 % des auteurs de la base de données Web of Science ont moins de 5 publications, de sorte que les auteurs les plus cités parmi les 4 millions inclus dans cette étude constituent une infime fraction.

## Alternatives

Une autre approche pour mesurer l’impact d’un chercheur repose sur les données d’utilisation, comme le nombre de téléchargements des éditeurs et l’analyse de la performance des citations, souvent au niveau des articles.

Dès 2004, le BMJ a publié le nombre de points de vue pour ses articles, ce qui s’est révélé quelque peu corrélé aux citations. En 2008, le Journal of Medical Internet Research a commencé à publier des opinions et des gazouillis. Ces "tweetations" se sont avérées être un bon indicateur des articles fortement cités, conduisant l’auteur à proposer un "facteur Twimpact", qui est le nombre de Tweets qu’il reçoit dans les sept premiers jours de publication, ainsi qu’un Twindex, qui est le centile de rang du facteur Twimpact d’un article.

En réponse aux préoccupations croissantes au sujet de l’utilisation inappropriée des facteurs d’impact des revues dans l’évaluation des résultats scientifiques et des scientifiques eux-mêmes, l’Université de Montréal, l’Imperial College de Londres, PLOS, eLife, EMBO Journal, La Royal Society, Nature and Science a proposé des mesures de la distribution des citations comme solution de rechange aux facteurs d’impact.

## Publications en libre accès

Les publications en libre accès sont accessibles gratuitement aux lecteurs. On s'attend donc à ce qu'elles soient citées plus fréquemment. Certaines études expérimentales et d'observation ont montré que les articles publiés dans des revues en libre accès ne sont pas plus cités, en moyenne, que ceux publiés dans des revues payantes, alors que d'autres études ont montré le contraire.

La preuve que les articles OA auto-archivés ("vert") sont plus cités que les articles non OA est un peu plus forte que la preuve que les revues OA ("or") sont plus citées que les revues non OA. Cela s'explique par le fait qu'un grand nombre des revues les plus citées aujourd'hui ne sont encore que des OA hybrides (l'auteur a la possibilité de payer pour l'or) et qu'un grand nombre de revues OA purement payantes sont soit de mauvaise qualité, soit carrément des "revues prédatrices" qui profitent de l'empressement des auteurs à publier ou à périr, ce qui fait baisser le nombre moyen de citations des revues OA.

## Développements récents

Un développement récent important dans la recherche sur l'impact des citations est la découverte de l'universalité, ou des modèles d'impact des citations qui sont valables dans différentes disciplines des sciences, des sciences sociales et des sciences humaines. Par exemple, il a été démontré que le nombre de citations reçues par une publication, une fois correctement rééchelonné par sa moyenne parmi les articles publiés dans la même discipline et la même année, suit une distribution log-normale universelle qui est la même dans toutes les disciplines. Cette constatation a suggéré une mesure universelle de l'impact des citations qui étend le h-index en redimensionnant correctement le nombre de citations et en recourant à des publications, mais le calcul d'une telle mesure universelle nécessite la collecte de nombreuses données et statistiques sur les citations pour chaque discipline et chaque année. Des outils de crowdsourcing social tels que Scholarometer ont été proposés pour répondre à ce besoin. Kaur et al. ont proposé une méthode statistique pour évaluer l'universalité des mesures d'impact des citations, c'est-à-dire leur capacité à comparer équitablement l'impact entre les domaines. Leur analyse identifie des mesures d'impact universelles, telles que le h-index normalisé par champ.

La recherche suggère que l'impact d'un article peut être, en partie, expliqué par des facteurs superficiels et pas seulement par les mérites scientifiques d'un article. Les facteurs dépendants du domaine sont généralement cités comme un problème à résoudre non seulement lorsque des comparaisons entre disciplines sont effectuées, mais aussi lorsque différents domaines de recherche d'une même discipline sont comparés. Par exemple, en médecine, parmi d'autres facteurs, le nombre d'auteurs, le nombre de références, la longueur de l'article et la présence de deux points dans le titre influencent l'impact. En sociologie, le nombre de références, la longueur de l'article et la longueur du titre font partie de ces facteurs. On constate également que les chercheurs adoptent un comportement éthiquement discutable afin de gonfler le nombre de citations reçues par leurs articles.

L'indexation automatisée des citations a changé la nature de la recherche sur l'analyse des citations, permettant d'analyser des millions de citations pour trouver des modèles à grande échelle et découvrir des connaissances. Le premier exemple d'indexation automatique des citations a été CiteSeer, suivi plus tard par Google Scholar. Plus récemment, des modèles avancés pour une analyse dynamique du vieillissement des citations ont été proposés. Ce dernier modèle est même utilisé comme outil prédictif pour déterminer les citations qui pourraient être obtenues à tout moment de la vie d'un corpus de publications.

Certains chercheurs proposent également que le taux de citation des revues sur Wikipédia, à côté de l'indice de citation traditionnel, "puisse être un bon indicateur de l'impact du travail dans le domaine de la psychologie."

Selon Mario Biagioli : "Toute métrique d'évaluation scientifique est vouée à être abusée. La loi de Goodhart stipule que lorsqu'une caractéristique de l'économie est choisie comme indicateur de l'économie, alors elle cesse inexorablement de fonctionner comme cet indicateur car les gens commencent à la jouer."

